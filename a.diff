diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/plan/nodes/datastream/DataStreamAggregate.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/plan/nodes/datastream/DataStreamAggregate.scala
index d36eb42..c75ca0b 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/plan/nodes/datastream/DataStreamAggregate.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/plan/nodes/datastream/DataStreamAggregate.scala
@@ -103,13 +103,11 @@ class DataStreamAggregate(
   }
 
   override def translateToPlan(
-                                tableEnv: StreamTableEnvironment,
-                                expectedType: Option[TypeInformation[Any]])
-  : DataStream[Any] = {
+      tableEnv: StreamTableEnvironment,
+      expectedType: Option[TypeInformation[Any]])
+    : DataStream[Any] = {
     val config = tableEnv.getConfig
-
     val groupingKeys = grouping.indices.toArray
-
     val inputDS = input.asInstanceOf[DataStreamRel].translateToPlan(
       tableEnv,
       // tell the input operator that this operator currently only supports Rows as input
@@ -117,8 +115,10 @@ class DataStreamAggregate(
 
     // get the output types
     val fieldTypes: Array[TypeInformation[_]] = getRowType.getFieldList.asScala
-      .map(field => FlinkTypeFactory.toTypeInfo(field.getType))
-      .toArray
+                                                .map(
+                                                  field => FlinkTypeFactory
+                                                           .toTypeInfo(field.getType))
+                                                .toArray
 
     val rowTypeInfo = new RowTypeInfo(fieldTypes)
 
@@ -130,15 +130,18 @@ class DataStreamAggregate(
       namedProperties)
 
     val prepareOpName = s"prepare select: ($aggString)"
+    val keyedAggOpName = s"groupBy: (${groupingToString(inputType, grouping)}), " +
+      s"window: ($window), " +
+      s"select: ($aggString)"
+    val nonKeyedAggOpName = s"window: ($window), select: ($aggString)"
 
     val (aggFieldIndexes, aggregates) =
       AggregateUtil.transformToAggregateFunctions(
         namedAggregates.map(_.getKey), inputType, grouping.length)
 
     val result: DataStream[Any] = {
-
-      // check all aggregates are support Partial aggregate
-      if (aggregates.map(_.supportPartial).forall(x => x)) {
+      // check whether all aggregates support partial aggregate
+      if (aggregates.forall(_.supportPartial)){
         // do Incremental Aggregation
         // add grouping fields, position keys in the input, and input type
         val (mapFunction,
@@ -152,16 +155,15 @@ class DataStreamAggregate(
           grouping, aggregates, aggFieldIndexes)
 
         val mappedInput = inputDS
-          .map(mapFunction)
-          .name(prepareOpName)
+                          .map(mapFunction)
+                          .name(prepareOpName)
 
         // grouped / keyed aggregation
         if (groupingKeys.length > 0) {
-          val aggOpName = s"groupBy: (${groupingToString(inputType, grouping)}), " +
-            s"window: ($window), " +
-            s"select: ($aggString)"
+
           val winFunction =
-            createWindowIncrementalAggregationFunction(aggregates,
+            createWindowIncrementalAggregationFunction(
+              aggregates,
               groupingOffsetMapping,
               aggOffsetMapping,
               getRowType.getFieldCount,
@@ -169,21 +171,20 @@ class DataStreamAggregate(
               window, namedProperties)
 
           val keyedStream = mappedInput.keyBy(groupingKeys: _*)
-
           val windowedStream = createKeyedWindowedStream(window, keyedStream)
-            .asInstanceOf[WindowedStream[Row, Tuple, DataStreamWindow]]
+                               .asInstanceOf[WindowedStream[Row, Tuple, DataStreamWindow]]
 
           windowedStream
-            .apply(reduceFunction, winFunction)
-            .returns(rowTypeInfo)
-            .name(aggOpName)
-            .asInstanceOf[DataStream[Any]]
+          .apply(reduceFunction, winFunction)
+          .returns(rowTypeInfo)
+          .name(keyedAggOpName)
+          .asInstanceOf[DataStream[Any]]
         }
         // global / non-keyed aggregation
         else {
-          val aggOpName = s"window: ($window), select: ($aggString)"
           val winFunction =
-            createAllWindowIncrementalAggregationFunction(aggregates,
+            createAllWindowIncrementalAggregationFunction(
+              aggregates,
               groupingOffsetMapping,
               aggOffsetMapping,
               getRowType.getFieldCount,
@@ -192,13 +193,12 @@ class DataStreamAggregate(
               namedProperties)
 
           val windowedStream = createNonKeyedWindowedStream(window, mappedInput)
-            .asInstanceOf[AllWindowedStream[Row, DataStreamWindow]]
-
+                               .asInstanceOf[AllWindowedStream[Row, DataStreamWindow]]
           windowedStream
-            .apply(reduceFunction, winFunction)
-            .returns(rowTypeInfo)
-            .name(aggOpName)
-            .asInstanceOf[DataStream[Any]]
+          .apply(reduceFunction, winFunction)
+          .returns(rowTypeInfo)
+          .name(nonKeyedAggOpName)
+          .asInstanceOf[DataStream[Any]]
         }
       }
       else {
@@ -210,41 +210,34 @@ class DataStreamAggregate(
           getRowType,
           grouping, aggregates, aggFieldIndexes)
         val mappedInput = inputDS
-          .map(mapFunction)
-          .name(prepareOpName)
+                          .map(mapFunction)
+                          .name(prepareOpName)
         // grouped / keyed aggregation
         if (groupingKeys.length > 0) {
-          val aggOpName = s"groupBy: (${groupingToString(inputType, grouping)}), " +
-            s"window: ($window), " +
-            s"select: ($aggString)"
           val aggregateFunction =
             createWindowAggregationFunction(window, namedProperties, groupReduceFunction)
 
           val keyedStream = mappedInput.keyBy(groupingKeys: _*)
-
           val windowedStream = createKeyedWindowedStream(window, keyedStream)
-            .asInstanceOf[WindowedStream[Row, Tuple, DataStreamWindow]]
-
+                               .asInstanceOf[WindowedStream[Row, Tuple, DataStreamWindow]]
           windowedStream
-            .apply(aggregateFunction)
-            .returns(rowTypeInfo)
-            .name(aggOpName)
-            .asInstanceOf[DataStream[Any]]
+          .apply(aggregateFunction)
+          .returns(rowTypeInfo)
+          .name(keyedAggOpName)
+          .asInstanceOf[DataStream[Any]]
         }
         // global / non-keyed aggregation
         else {
-          val aggOpName = s"window: ($window), select: ($aggString)"
           val aggregateFunction =
             createAllWindowAggregationFunction(window, namedProperties, groupReduceFunction)
 
           val windowedStream = createNonKeyedWindowedStream(window, mappedInput)
-            .asInstanceOf[AllWindowedStream[Row, DataStreamWindow]]
-
+                               .asInstanceOf[AllWindowedStream[Row, DataStreamWindow]]
           windowedStream
-            .apply(aggregateFunction)
-            .returns(rowTypeInfo)
-            .name(aggOpName)
-            .asInstanceOf[DataStream[Any]]
+          .apply(aggregateFunction)
+          .returns(rowTypeInfo)
+          .name(nonKeyedAggOpName)
+          .asInstanceOf[DataStream[Any]]
         }
       }
     }
@@ -252,18 +245,18 @@ class DataStreamAggregate(
     expectedType match {
       case Some(typeInfo) if typeInfo.getTypeClass != classOf[Row] =>
         val mapName = s"convert: (${getRowType.getFieldNames.asScala.toList.mkString(", ")})"
-        result.map(getConversionMapper(
-          config = config,
-          nullableInput = false,
-          inputType = rowTypeInfo.asInstanceOf[TypeInformation[Any]],
-          expectedType = expectedType.get,
-          conversionOperatorName = "DataStreamAggregateConversion",
-          fieldNames = getRowType.getFieldNames.asScala
-        ))
-          .name(mapName)
+        result.map(
+          getConversionMapper(
+            config = config,
+            nullableInput = false,
+            inputType = rowTypeInfo.asInstanceOf[TypeInformation[Any]],
+            expectedType = expectedType.get,
+            conversionOperatorName = "DataStreamAggregateConversion",
+            fieldNames = getRowType.getFieldNames.asScala
+          ))
+        .name(mapName)
       case _ => result
     }
-    result
   }
 
 }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateAllWindowFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateAllWindowFunction.scala
index b1a0058..7d1e0e7 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateAllWindowFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateAllWindowFunction.scala
@@ -27,8 +27,10 @@ import org.apache.flink.streaming.api.functions.windowing.RichAllWindowFunction
 import org.apache.flink.streaming.api.windowing.windows.Window
 import org.apache.flink.util.Collector
 
-class AggregateAllWindowFunction[W<:Window](groupReduceFunction: RichGroupReduceFunction[Row, Row])
-    extends RichAllWindowFunction[Row, Row, W] {
+class AggregateAllWindowFunction[W <: Window](
+    groupReduceFunction: RichGroupReduceFunction[Row,
+    Row])
+  extends RichAllWindowFunction[Row, Row, W] {
 
   override def open(parameters: Configuration): Unit = {
     groupReduceFunction.open(parameters)
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateMapFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateMapFunction.scala
index d848d21..9b834eb 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateMapFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateMapFunction.scala
@@ -30,9 +30,9 @@ class AggregateMapFunction[IN, OUT](
     private val groupingKeys: Array[Int],
     @transient private val returnType: TypeInformation[OUT])
     extends RichMapFunction[IN, OUT] with ResultTypeQueryable[OUT] {
-  
+
   private var output: Row = _
-  
+
   override def open(config: Configuration) {
     Preconditions.checkNotNull(aggregates)
     Preconditions.checkNotNull(aggFields)
@@ -43,7 +43,7 @@ class AggregateMapFunction[IN, OUT](
   }
 
   override def map(value: IN): OUT = {
-    
+
     val input = value.asInstanceOf[Row]
     for (i <- 0 until aggregates.length) {
       val fieldValue = input.productElement(aggFields(i))
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateUtil.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateUtil.scala
index ebc402a..943d4e2 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateUtil.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/AggregateUtil.scala
@@ -61,73 +61,77 @@ object AggregateUtil {
    * }}}
    *
    */
-    def createOperatorFunctionsForAggregates(
+  def createOperatorFunctionsForAggregates(
       namedAggregates: Seq[CalcitePair[AggregateCall, String]],
       inputType: RelDataType,
       outputType: RelDataType,
       groupings: Array[Int])
     : (MapFunction[Any, Row], RichGroupReduceFunction[Row, Row]) = {
 
-       val (aggFieldIndexes, aggregates)  =
-           transformToAggregateFunctions(namedAggregates.map(_.getKey),
-             inputType, groupings.length)
-
-        createOperatorFunctionsForAggregates(namedAggregates,
-          inputType,
-          outputType,
-          groupings,
-          aggregates,aggFieldIndexes)
-    }
+    val (aggFieldIndexes, aggregates) =
+      transformToAggregateFunctions(
+        namedAggregates.map(_.getKey),
+        inputType, groupings.length)
+
+    createOperatorFunctionsForAggregates(
+      namedAggregates,
+      inputType,
+      outputType,
+      groupings,
+      aggregates, aggFieldIndexes)
+  }
 
-    def createOperatorFunctionsForAggregates(
-        namedAggregates: Seq[CalcitePair[AggregateCall, String]],
-        inputType: RelDataType,
-        outputType: RelDataType,
-        groupings: Array[Int],
-        aggregates:Array[Aggregate[_ <: Any]],
-        aggFieldIndexes:Array[Int])
-    : (MapFunction[Any, Row], RichGroupReduceFunction[Row, Row])= {
+  def createOperatorFunctionsForAggregates(
+      namedAggregates: Seq[CalcitePair[AggregateCall, String]],
+      inputType: RelDataType,
+      outputType: RelDataType,
+      groupings: Array[Int],
+      aggregates: Array[Aggregate[_ <: Any]],
+      aggFieldIndexes: Array[Int])
+    : (MapFunction[Any, Row], RichGroupReduceFunction[Row, Row]) = {
 
-      val mapFunction = createAggregateMapFunction(aggregates,
-                        aggFieldIndexes, groupings, inputType)
+    val mapFunction = createAggregateMapFunction(
+      aggregates,
+      aggFieldIndexes, groupings, inputType)
 
-      // the mapping relation between field index of intermediate aggregate Row and output Row.
-      val groupingOffsetMapping = getGroupKeysMapping(inputType, outputType, groupings)
+    // the mapping relation between field index of intermediate aggregate Row and output Row.
+    val groupingOffsetMapping = getGroupKeysMapping(inputType, outputType, groupings)
 
-      // the mapping relation between aggregate function index in list and its corresponding
-      // field index in output Row.
-      val aggOffsetMapping = getAggregateMapping(namedAggregates, outputType)
+    // the mapping relation between aggregate function index in list and its corresponding
+    // field index in output Row.
+    val aggOffsetMapping = getAggregateMapping(namedAggregates, outputType)
 
-      if (groupingOffsetMapping.length != groupings.length ||
-        aggOffsetMapping.length != namedAggregates.length) {
-        throw new TableException("Could not find output field in input data type " +
+    if (groupingOffsetMapping.length != groupings.length ||
+      aggOffsetMapping.length != namedAggregates.length) {
+      throw new TableException(
+        "Could not find output field in input data type " +
           "or aggregate functions.")
-      }
+    }
 
-      val allPartialAggregate = aggregates.map(_.supportPartial).forall(x => x)
+    val allPartialAggregate = aggregates.forall(_.supportPartial)
 
-      val intermediateRowArity = groupings.length +
-                        aggregates.map(_.intermediateDataType.length).sum
+    val intermediateRowArity = groupings.length +
+      aggregates.map(_.intermediateDataType.length).sum
 
-      val reduceGroupFunction =
-        if (allPartialAggregate) {
-          new AggregateReduceCombineFunction(
-            aggregates,
-            groupingOffsetMapping,
-            aggOffsetMapping,
-            intermediateRowArity,
-            outputType.getFieldCount)
-        }
-        else {
-          new AggregateReduceGroupFunction(
-            aggregates,
-            groupingOffsetMapping,
-            aggOffsetMapping,
-            intermediateRowArity,
-            outputType.getFieldCount)
-        }
+    val reduceGroupFunction =
+      if (allPartialAggregate) {
+        new AggregateReduceCombineFunction(
+          aggregates,
+          groupingOffsetMapping,
+          aggOffsetMapping,
+          intermediateRowArity,
+          outputType.getFieldCount)
+      }
+      else {
+        new AggregateReduceGroupFunction(
+          aggregates,
+          groupingOffsetMapping,
+          aggOffsetMapping,
+          intermediateRowArity,
+          outputType.getFieldCount)
+      }
 
-      (mapFunction, reduceGroupFunction)
+    (mapFunction, reduceGroupFunction)
   }
 
   /**
@@ -151,6 +155,9 @@ object AggregateUtil {
     *                               sum(y) aggOffsetInRow = 4
     * }}}
     *
+    * @return (mapFunction, reduceFunction,
+    *         groupingOffsetMapping, aggOffsetMapping,
+    *         intermediateRowArity)
     */
   def createOperatorFunctionsForIncrementalAggregates(
       namedAggregates: Seq[CalcitePair[AggregateCall, String]],
@@ -159,7 +166,7 @@ object AggregateUtil {
       groupings: Array[Int],
       aggregates:Array[Aggregate[_ <: Any]],
       aggFieldIndexes:Array[Int])
-  : (MapFunction[Any, Row], ReduceFunction[Row],
+    : (MapFunction[Any, Row], ReduceFunction[Row],
               Array[(Int, Int)],Array[(Int, Int)],Int) = {
 
     val mapFunction = createAggregateMapFunction(aggregates, aggFieldIndexes, groupings, inputType)
@@ -183,10 +190,11 @@ object AggregateUtil {
 
   }
 
-  private def createAggregateMapFunction(aggregates:Array[Aggregate[_ <: Any]],
-         aggFieldIndexes:Array[Int],
-         groupings: Array[Int],
-         inputType: RelDataType): MapFunction[Any, Row] ={
+  private def createAggregateMapFunction(
+      aggregates: Array[Aggregate[_ <: Any]],
+      aggFieldIndexes: Array[Int],
+      groupings: Array[Int],
+      inputType: RelDataType): MapFunction[Any, Row] = {
 
     val mapReturnType: RowTypeInfo =
       createAggregateBufferDataType(groupings, aggregates, inputType)
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllTimeWindowFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllTimeWindowFunction.scala
index 0e39d97..9927ce5 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllTimeWindowFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllTimeWindowFunction.scala
@@ -33,15 +33,16 @@ import org.apache.flink.util.Collector
   *                         index in output Row.
   */
 class IncrementalAggregateAllTimeWindowFunction(
-      private val aggregates: Array[Aggregate[_ <: Any]],
-      private val groupKeysMapping: Array[(Int, Int)],
-      private val aggregateMapping: Array[(Int, Int)],
-      private val finalRowArity: Int,
-      private val windowStartPos: Option[Int],
-      private val windowEndPos: Option[Int])
-               extends IncrementalAggregateAllWindowFunction[TimeWindow](aggregates,
-                 groupKeysMapping,
-                 aggregateMapping,finalRowArity) {
+    private val aggregates: Array[Aggregate[_ <: Any]],
+    private val groupKeysMapping: Array[(Int, Int)],
+    private val aggregateMapping: Array[(Int, Int)],
+    private val finalRowArity: Int,
+    private val windowStartPos: Option[Int],
+    private val windowEndPos: Option[Int])
+  extends IncrementalAggregateAllWindowFunction[TimeWindow](
+    aggregates,
+    groupKeysMapping,
+    aggregateMapping, finalRowArity) {
 
   private var collector: TimeWindowPropertyCollector = _
 
@@ -50,13 +51,14 @@ class IncrementalAggregateAllTimeWindowFunction(
     super.open(parameters)
   }
 
-  override def apply(window: TimeWindow,
-                     records: Iterable[Row],
-                     out: Collector[Row]): Unit = {
+  override def apply(
+    window: TimeWindow,
+    records: Iterable[Row],
+    out: Collector[Row]): Unit = {
     // set collector and window
     collector.wrappedCollector = out
     collector.timeWindow = window
 
-    super.apply(window,records,collector)
+    super.apply(window, records, collector)
   }
 }
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllWindowFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllWindowFunction.scala
index 83ef7bf..f95fb65 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllWindowFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateAllWindowFunction.scala
@@ -35,10 +35,10 @@ import org.apache.flink.util.{Collector, Preconditions}
   *                         index in output Row.
   */
 class IncrementalAggregateAllWindowFunction[W <: Window](
-      private val aggregates: Array[Aggregate[_ <: Any]],
-      private val groupKeysMapping: Array[(Int, Int)],
-      private val aggregateMapping: Array[(Int, Int)],
-      private val finalRowArity: Int) extends RichAllWindowFunction[Row, Row, W] {
+    private val aggregates: Array[Aggregate[_ <: Any]],
+    private val groupKeysMapping: Array[(Int, Int)],
+    private val aggregateMapping: Array[(Int, Int)],
+    private val finalRowArity: Int) extends RichAllWindowFunction[Row, Row, W] {
 
   private var output: Row = _
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateReduceFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateReduceFunction.scala
index f796b6b..d875e24 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateReduceFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateReduceFunction.scala
@@ -29,9 +29,9 @@ import org.apache.flink.util.Preconditions
   *                         and output Row.
   */
 class IncrementalAggregateReduceFunction(
-      private val aggregates: Array[Aggregate[_]],
-      private val groupKeysMapping: Array[(Int, Int)],
-      private val intermediateRowArity: Int)extends ReduceFunction[Row] {
+    private val aggregates: Array[Aggregate[_]],
+    private val groupKeysMapping: Array[(Int, Int)],
+    private val intermediateRowArity: Int)extends ReduceFunction[Row] {
 
   Preconditions.checkNotNull(aggregates)
   Preconditions.checkNotNull(groupKeysMapping)
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateTimeWindowFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateTimeWindowFunction.scala
index b148db6..0e0b69e 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateTimeWindowFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateTimeWindowFunction.scala
@@ -35,15 +35,15 @@ import org.apache.flink.util.Collector
   *                         index in output Row.
   */
 class IncrementalAggregateTimeWindowFunction(
-     private val aggregates: Array[Aggregate[_ <: Any]],
-     private val groupKeysMapping: Array[(Int, Int)],
-     private val aggregateMapping: Array[(Int, Int)],
-     private val finalRowArity: Int,
-     private val windowStartPos: Option[Int],
-     private val windowEndPos: Option[Int])
+    private val aggregates: Array[Aggregate[_ <: Any]],
+    private val groupKeysMapping: Array[(Int, Int)],
+    private val aggregateMapping: Array[(Int, Int)],
+    private val finalRowArity: Int,
+    private val windowStartPos: Option[Int],
+    private val windowEndPos: Option[Int])
   extends IncrementalAggregateWindowFunction[TimeWindow](aggregates,
-    groupKeysMapping,
-    aggregateMapping,finalRowArity) {
+     groupKeysMapping,
+     aggregateMapping,finalRowArity) {
 
   private var collector: TimeWindowPropertyCollector = _
 
diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateWindowFunction.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateWindowFunction.scala
index e46053d..d5f2ba3 100644
--- a/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateWindowFunction.scala
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/api/table/runtime/aggregate/IncrementalAggregateWindowFunction.scala
@@ -36,10 +36,10 @@ import org.apache.flink.util.{Collector, Preconditions}
   *                         index in output Row.
   */
 class IncrementalAggregateWindowFunction[W <:Window ](
-      private val aggregates: Array[Aggregate[_ <: Any]],
-      private val groupKeysMapping: Array[(Int, Int)],
-      private val aggregateMapping: Array[(Int, Int)],
-      private val finalRowArity: Int)
+    private val aggregates: Array[Aggregate[_ <: Any]],
+    private val groupKeysMapping: Array[(Int, Int)],
+    private val aggregateMapping: Array[(Int, Int)],
+    private val finalRowArity: Int)
   extends RichWindowFunction[Row, Row, Tuple, W] {
 
   private var output: Row = _
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/api/scala/stream/table/AggregationsITCase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/api/scala/stream/table/AggregationsITCase.scala
index 091e434..46b84d1 100644
--- a/flink-libraries/flink-table/src/test/scala/org/apache/flink/api/scala/stream/table/AggregationsITCase.scala
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/api/scala/stream/table/AggregationsITCase.scala
@@ -40,7 +40,7 @@ import scala.collection.mutable
 class AggregationsITCase extends StreamingMultipleProgramsTestBase {
 
   val data = List(
-    (1L, 2, "Hi"),
+    (1L, 1, "Hi"),
     (2L, 2, "Hello"),
     (4L, 2, "Hello"),
     (8L, 3, "Hello world"),
@@ -57,15 +57,15 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
     val table = stream.toTable(tEnv, 'long, 'int, 'string)
 
     val windowedTable = table
-      .groupBy('string)
-      .window(Slide over 2.rows every 1.rows)
-      .select('string, 'int.count)
+                        .groupBy('string)
+                        .window(Slide over 2.rows every 1.rows)
+                        .select('string, 'int.count, 'int.avg)
 
     val results = windowedTable.toDataStream[Row]
     results.addSink(new StreamITCase.StringSink)
     env.execute()
 
-    val expected = Seq("Hello world,1", "Hello world,2", "Hello,1", "Hello,2", "Hi,1")
+    val expected = Seq("Hello world,1,3", "Hello world,2,3", "Hello,1,2", "Hello,2,2", "Hi,1,1")
     assertEquals(expected.sorted, StreamITCase.testResults.sorted)
   }
 
@@ -77,14 +77,14 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
     StreamITCase.testResults = mutable.MutableList()
 
     val stream = env
-      .fromCollection(data)
-      .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
+                 .fromCollection(data)
+                 .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
     val table = stream.toTable(tEnv, 'long, 'int, 'string)
 
     val windowedTable = table
-      .groupBy('string)
-      .window(Session withGap 7.milli on 'rowtime)
-      .select('string, 'int.count)
+                        .groupBy('string)
+                        .window(Session withGap 7.milli on 'rowtime)
+                        .select('string, 'int.count)
 
     val results = windowedTable.toDataStream[Row]
     results.addSink(new StreamITCase.StringSink)
@@ -105,8 +105,8 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
     val table = stream.toTable(tEnv, 'long, 'int, 'string)
 
     val windowedTable = table
-      .window(Tumble over 2.rows)
-      .select('int.count)
+                        .window(Tumble over 2.rows)
+                        .select('int.count)
 
     val results = windowedTable.toDataStream[Row]
     results.addSink(new StreamITCase.StringSink)
@@ -124,24 +124,24 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
     StreamITCase.testResults = mutable.MutableList()
 
     val stream = env
-      .fromCollection(data)
-      .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
+                 .fromCollection(data)
+                 .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
     val table = stream.toTable(tEnv, 'long, 'int, 'string)
 
     val windowedTable = table
-      .groupBy('string)
-      .window(Tumble over 5.milli on 'rowtime as 'w)
-      .select('string, 'int.count, 'w.start, 'w.end)
+                        .groupBy('string)
+                        .window(Tumble over 5.milli on 'rowtime as 'w)
+                        .select('string, 'int.count,'int.avg, 'w.start, 'w.end)
 
     val results = windowedTable.toDataStream[Row]
     results.addSink(new StreamITCase.StringSink)
     env.execute()
 
     val expected = Seq(
-      "Hello world,1,1970-01-01 00:00:00.005,1970-01-01 00:00:00.01",
-      "Hello world,1,1970-01-01 00:00:00.015,1970-01-01 00:00:00.02",
-      "Hello,2,1970-01-01 00:00:00.0,1970-01-01 00:00:00.005",
-      "Hi,1,1970-01-01 00:00:00.0,1970-01-01 00:00:00.005")
+      "Hello world,1,3,1970-01-01 00:00:00.005,1970-01-01 00:00:00.01",
+      "Hello world,1,3,1970-01-01 00:00:00.015,1970-01-01 00:00:00.02",
+      "Hello,2,2,1970-01-01 00:00:00.0,1970-01-01 00:00:00.005",
+      "Hi,1,1,1970-01-01 00:00:00.0,1970-01-01 00:00:00.005")
     assertEquals(expected.sorted, StreamITCase.testResults.sorted)
   }
 
@@ -153,14 +153,14 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
     StreamITCase.testResults = mutable.MutableList()
 
     val stream = env
-      .fromCollection(data)
-      .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
+                 .fromCollection(data)
+                 .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
     val table = stream.toTable(tEnv, 'long, 'int, 'string)
 
     val windowedTable = table
-      .groupBy('string)
-      .window(Slide over 10.milli every 5.milli on 'rowtime as 'w)
-      .select('string, 'int.count, 'w.start, 'w.end, 'w.start)
+                        .groupBy('string)
+                        .window(Slide over 10.milli every 5.milli on 'rowtime as 'w)
+                        .select('string, 'int.count, 'w.start, 'w.end, 'w.start)
 
     val results = windowedTable.toDataStream[Row]
     results.addSink(new StreamITCase.StringSink)
@@ -177,60 +177,7 @@ class AggregationsITCase extends StreamingMultipleProgramsTestBase {
       "Hi,1,1970-01-01 00:00:00.0,1970-01-01 00:00:00.01,1970-01-01 00:00:00.0")
     assertEquals(expected.sorted, StreamITCase.testResults.sorted)
   }
-
-  @Test
-  def testProcessingTimeSlidingGroupWindowOverCountWithAVG(): Unit = {
-    val env = StreamExecutionEnvironment.getExecutionEnvironment
-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
-    val tEnv = TableEnvironment.getTableEnvironment(env)
-    StreamITCase.testResults = mutable.MutableList()
-
-    val stream = env.fromCollection(data)
-    val table = stream.toTable(tEnv, 'long, 'int, 'string)
-
-    val windowedTable = table
-      .groupBy('string)
-      .window(Slide over 4.rows every 2.rows)
-      .select('string, 'int.avg)
-
-    val results = windowedTable.toDataStream[Row]
-    results.addSink(new StreamITCase.StringSink)
-    env.execute()
-
-    val expected = Seq("Hello world,3","Hello,2")
-    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
-  }
-
-  @Test
-  def testEventTimeTumblingWindowWithAVG(): Unit = {
-    val env = StreamExecutionEnvironment.getExecutionEnvironment
-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
-    val tEnv = TableEnvironment.getTableEnvironment(env)
-    StreamITCase.testResults = mutable.MutableList()
-
-    val stream = env
-      .fromCollection(data)
-      .assignTimestampsAndWatermarks(new TimestampWithEqualWatermark())
-    val table = stream.toTable(tEnv, 'long, 'int, 'string)
-
-    val windowedTable = table
-      .groupBy('string)
-      .window(Tumble over 5.milli on 'rowtime as 'w)
-      .select('string, 'int.avg)
-
-    val results = windowedTable.toDataStream[Row]
-    results.addSink(new StreamITCase.StringSink)
-    env.execute()
-
-    val expected = Seq(
-      "Hello world,3",
-      "Hello world,3",
-      "Hello,2",
-      "Hi,2")
-    assertEquals(expected.sorted, StreamITCase.testResults.sorted)
-  }
 }
-
 object GroupWindowITCase {
   class TimestampWithEqualWatermark extends AssignerWithPunctuatedWatermarks[(Long, Int, String)] {
 
